import os
import sys
import importlib.util
import logging
import shutil
from modelscope import snapshot_download

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# æ¨¡å‹é…ç½®åˆ—è¡¨
MODELS_CONFIG = [
    {
        "name": "bge-large-zh-v1.5",
        "model_id": "AI-ModelScope/bge-large-zh-v1.5",
        "local_dir": r"E:\github_project\models\bge-large-zh-v1.5",
        "revision": "master",
        "is_embedding": True,
        "need_download": False,
    },
    {
        "name": "chatglm3-6b",
        "model_id": "ZhipuAI/chatglm3-6b",
        "local_dir": r"E:\github_project\models\chatglm3-6b",
        "revision": "v1.0.0",
        "is_llm": True,
        "need_download": False,
    },
    {
        "name": "bge-reranker-large",
        "model_id": "Xorbits/bge-reranker-large",
        "local_dir": r"E:\github_project\models\bge-reranker-large",
        "revision": "master",
        "is_reranker": True,
        "need_download": False,
    },
    {
        "name": "m3e-base",
        "model_id": "AI-ModelScope/m3e-base",
        "local_dir": r"E:\github_project\models\m3e-base",
        "revision": "master",
        "is_embedding": True,
        "need_download": True,
    },
    {
        "name": "chatglm4-9b-chat",
        "model_id": "ZhipuAI/chatglm4-9b-chat",
        "local_dir": r"E:\github_project\models\chatglm4-9b-chat",
        "revision": "master",
        "is_llm": True,
        "need_download": False,
    },
]


def check_model_exists(local_dir):
    """
    æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    if not os.path.exists(local_dir):
        return False

    # éå†ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æ–‡ä»¶
    all_files = []
    for root, dirs, files in os.walk(local_dir):
        all_files.extend(files)

    # åŸºç¡€æ¨¡å‹æ–‡ä»¶æ£€æŸ¥

    # è‡³å°‘éœ€è¦config.json
    if "config.json" not in all_files:
        logger.warning(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: config.json in {local_dir}")
        return False

    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
    has_model_weight = False
    # æƒ…å†µ1ï¼šå•ä¸ªæ¨¡å‹æ–‡ä»¶
    single_model_files = ["model.safetensors", "pytorch_model.bin"]
    for weight_file in single_model_files:
        if weight_file in all_files:
            has_model_weight = True
            break

    # æƒ…å†µ2ï¼šåˆ†ç‰‡æ¨¡å‹æ–‡ä»¶ï¼ˆpytorch_model-*-of-*.binï¼‰
    if not has_model_weight:
        has_sharded_model = any(
            "pytorch_model-" in file and "-of-" in file for file in all_files
        )
        has_index_file = "pytorch_model.bin.index.json" in all_files
        if has_sharded_model and has_index_file:
            has_model_weight = True

    if not has_model_weight:
        logger.warning(f"ç¼ºå°‘æ¨¡å‹æƒé‡æ–‡ä»¶ in {local_dir}")
        logger.warning(f"å½“å‰ç›®å½•æ–‡ä»¶: {all_files}")
        return False

    # æ£€æŸ¥æ˜¯å¦åŒ…å«tokenizerç›¸å…³æ–‡ä»¶ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰
    tokenizer_files = [
        "tokenizer.json",
        "tokenizer_config.json",
        "vocab.txt",
        "sentencepiece.bpe.model",
        "tokenizer.model",
    ]
    has_tokenizer = any(
        tokenizer_file in all_files for tokenizer_file in tokenizer_files
    )
    if not has_tokenizer:
        logger.warning(f"ç¼ºå°‘tokenizeræ–‡ä»¶: {tokenizer_files} in {local_dir}")

    logger.info(f"æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡: {local_dir}")
    return True


def test_model_import(model_config, verify_level="full"):
    """
    æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¯¼å…¥å’Œæ­£å¸¸å·¥ä½œ

    Args:
        model_config (dict): æ¨¡å‹é…ç½®
        verify_level (str): éªŒè¯çº§åˆ«ï¼Œå¯é€‰å€¼ï¼š
            - "basic": ä»…æµ‹è¯•å¯¼å…¥
            - "full": æµ‹è¯•å¯¼å…¥å’ŒåŠŸèƒ½

    Returns:
        tuple: (import_success, func_test_success)
            - import_success: bool, æ¨¡å‹æ˜¯å¦æˆåŠŸå¯¼å…¥
            - func_test_success: bool, åŠŸèƒ½æµ‹è¯•æ˜¯å¦æˆåŠŸ
    """
    import time

    try:
        import torch
        from transformers import (
            AutoModel,
            AutoTokenizer,
            AutoModelForSequenceClassification,
        )

        logger.info(f"æµ‹è¯•å¯¼å…¥æ¨¡å‹: {model_config['name']}")

        # é€šç”¨å‚æ•°
        model_kwargs = {"trust_remote_code": True}

        # æ£€æŸ¥accelerateåº“æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœå¯ç”¨åˆ™ä½¿ç”¨device_map="auto"ä¼˜åŒ–å¤§æ¨¡å‹åŠ è½½
        try:
            import accelerate

            model_kwargs["device_map"] = "auto"
            logger.info(f"âœ“ accelerateåº“å¯ç”¨ï¼Œä½¿ç”¨device_map='auto'ä¼˜åŒ–æ¨¡å‹åŠ è½½")
        except ImportError:
            logger.info(
                f"âš ï¸ accelerateåº“ä¸å¯ç”¨ï¼Œä¸ä½¿ç”¨device_map='auto' (å¯é€šè¿‡ 'pip install accelerate' å®‰è£…ä»¥ä¼˜åŒ–å¤§æ¨¡å‹åŠ è½½)"
            )

        # ä»…å¯¼å…¥tokenizerè¿›è¡ŒåŸºæœ¬æµ‹è¯•
        tokenizer = AutoTokenizer.from_pretrained(
            model_config["local_dir"], trust_remote_code=True
        )
        logger.info(f"âœ“ æˆåŠŸå¯¼å…¥tokenizer: {model_config['name']}")

        # å°è¯•å¯¼å…¥æ¨¡å‹
        if model_config.get("is_llm"):
            # æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹
            model = AutoModel.from_pretrained(model_config["local_dir"], **model_kwargs)
            logger.info(f"âœ“ æˆåŠŸå¯¼å…¥LLMæ¨¡å‹: {model_config['name']}")

            # åŠŸèƒ½æµ‹è¯•
            func_test_success = True
            if verify_level == "full":
                logger.info(f"å¼€å§‹æµ‹è¯•LLMç”ŸæˆåŠŸèƒ½: {model_config['name']}")
                test_input = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
                start_time = time.time()

                try:
                    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
                    inputs = tokenizer(test_input, return_tensors="pt")
                    outputs = model.generate(
                        **inputs, max_new_tokens=20, temperature=0.7, top_p=0.9
                    )
                    generated_text = tokenizer.decode(
                        outputs[0], skip_special_tokens=True
                    )

                    end_time = time.time()
                    gen_time = end_time - start_time

                    logger.info(f"âœ“ LLMç”ŸæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡: {model_config['name']}")
                    logger.info(f"  æµ‹è¯•è¾“å…¥: {test_input}")
                    logger.info(f"  ç”Ÿæˆè¾“å‡º: {generated_text}")
                    logger.info(f"  ç”Ÿæˆæ—¶é—´: {gen_time:.2f} ç§’")
                except Exception as func_error:
                    func_test_success = False
                    logger.error(
                        f"âš ï¸ LLMç”ŸæˆåŠŸèƒ½æµ‹è¯•å¤±è´¥: {model_config['name']}, é”™è¯¯: {func_error}"
                    )
                    logger.error(
                        f"  ğŸ’¡ æç¤º: åŠŸèƒ½æµ‹è¯•å¤±è´¥ä¸å½±å“æ¨¡å‹ä½¿ç”¨ï¼Œå¯èƒ½æ˜¯ç¯å¢ƒæˆ–å‚æ•°é—®é¢˜"
                    )

            return True, func_test_success

        elif model_config.get("is_embedding"):
            # æµ‹è¯•åµŒå…¥æ¨¡å‹
            model = AutoModel.from_pretrained(model_config["local_dir"], **model_kwargs)
            logger.info(f"âœ“ æˆåŠŸå¯¼å…¥åµŒå…¥æ¨¡å‹: {model_config['name']}")

            # åŠŸèƒ½æµ‹è¯•
            func_test_success = True
            if verify_level == "full":
                logger.info(f"å¼€å§‹æµ‹è¯•åµŒå…¥ç”ŸæˆåŠŸèƒ½: {model_config['name']}")
                test_sentence = "è¿™æ˜¯ä¸€ä¸ªåµŒå…¥æµ‹è¯•å¥å­"
                start_time = time.time()

                try:
                    # ç”ŸæˆåµŒå…¥
                    inputs = tokenizer(
                        test_sentence,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                    )
                    with torch.no_grad():
                        outputs = model(**inputs)
                        embedding = (
                            outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
                        )

                    end_time = time.time()
                    embed_time = end_time - start_time

                    logger.info(f"âœ“ åµŒå…¥ç”ŸæˆåŠŸèƒ½æµ‹è¯•é€šè¿‡: {model_config['name']}")
                    logger.info(f"  æµ‹è¯•å¥å­: {test_sentence}")
                    logger.info(f"  åµŒå…¥ç»´åº¦: {len(embedding)}")
                    logger.info(f"  ç”Ÿæˆæ—¶é—´: {embed_time:.2f} ç§’")
                    logger.info(f"  åµŒå…¥ç¤ºä¾‹: {embedding[:5]}...")
                except Exception as func_error:
                    func_test_success = False
                    logger.error(
                        f"âš ï¸ åµŒå…¥ç”ŸæˆåŠŸèƒ½æµ‹è¯•å¤±è´¥: {model_config['name']}, é”™è¯¯: {func_error}"
                    )
                    logger.error(
                        f"  ğŸ’¡ æç¤º: åŠŸèƒ½æµ‹è¯•å¤±è´¥ä¸å½±å“æ¨¡å‹ä½¿ç”¨ï¼Œå¯èƒ½æ˜¯ç¯å¢ƒæˆ–å‚æ•°é—®é¢˜"
                    )

            return True, func_test_success

        elif model_config.get("is_reranker"):
            # æµ‹è¯•é‡æ’åºæ¨¡å‹
            model = AutoModelForSequenceClassification.from_pretrained(
                model_config["local_dir"], **model_kwargs
            )
            logger.info(f"âœ“ æˆåŠŸå¯¼å…¥é‡æ’åºæ¨¡å‹: {model_config['name']}")

            # åŠŸèƒ½æµ‹è¯•
            func_test_success = True
            if verify_level == "full":
                logger.info(f"å¼€å§‹æµ‹è¯•é‡æ’åºåŠŸèƒ½: {model_config['name']}")
                query = "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
                docs = [
                    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚",
                    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸã€‚",
                    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªæ–¹æ³•ã€‚",
                    "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸã€‚",
                ]
                start_time = time.time()

                try:
                    # ç”Ÿæˆæ’åºåˆ†æ•°
                    inputs = tokenizer(
                        [query] * len(docs),
                        docs,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                    )
                    with torch.no_grad():
                        outputs = model(**inputs)
                        scores = outputs.logits.squeeze().tolist()

                    end_time = time.time()
                    rank_time = end_time - start_time

                    # æ’åºç»“æœ
                    ranked_docs = sorted(
                        zip(docs, scores), key=lambda x: x[1], reverse=True
                    )

                    logger.info(f"âœ“ é‡æ’åºåŠŸèƒ½æµ‹è¯•é€šè¿‡: {model_config['name']}")
                    logger.info(f"  æŸ¥è¯¢: {query}")
                    logger.info(f"  æµ‹è¯•æ–‡æ¡£æ•°: {len(docs)}")
                    logger.info(f"  å¤„ç†æ—¶é—´: {rank_time:.2f} ç§’")
                    logger.info(f"  æ’åºç»“æœ:")
                    for i, (doc, score) in enumerate(ranked_docs, 1):
                        logger.info(f"    {i}. åˆ†æ•°: {score:.4f} | {doc}")
                except Exception as func_error:
                    func_test_success = False
                    logger.error(
                        f"âš ï¸ é‡æ’åºåŠŸèƒ½æµ‹è¯•å¤±è´¥: {model_config['name']}, é”™è¯¯: {func_error}"
                    )
                    logger.error(
                        f"  ğŸ’¡ æç¤º: åŠŸèƒ½æµ‹è¯•å¤±è´¥ä¸å½±å“æ¨¡å‹ä½¿ç”¨ï¼Œå¯èƒ½æ˜¯ç¯å¢ƒæˆ–å‚æ•°é—®é¢˜"
                    )

            return True, func_test_success

        else:
            logger.warning(f"âš ï¸  æœªçŸ¥æ¨¡å‹ç±»å‹: {model_config['name']}")
            return True, False
    except Exception as e:
        logger.error(f"âœ— å¯¼å…¥æ¨¡å‹å¤±è´¥: {model_config['name']}, é”™è¯¯: {e}")
        return False, False


def download_model(model_config):
    """
    ä¸‹è½½æ¨¡å‹
    """
    model_dir = model_config["local_dir"]

    try:
        logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_config['name']}")
        logger.info(f"æ¨¡å‹ID: {model_config['model_id']}")
        logger.info(f"ä¿å­˜è·¯å¾„: {model_dir}")

        # è°ƒç”¨snapshot_downloadä¸‹è½½æ¨¡å‹
        downloaded_dir = snapshot_download(
            model_config["model_id"],
            revision=model_config["revision"],
            cache_dir=r"E:\github_project\models",
            local_dir=model_dir,
        )

        logger.info(
            f"âœ“ æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_config['name']}, ä¿å­˜è·¯å¾„: {downloaded_dir}"
        )
        return True
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âœ— æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_config['name']}, é”™è¯¯: {e}")

        # ç‰¹æ®Šå¤„ç†ModelScopeå¹³å°404é”™è¯¯
        if "not exists on either" in error_msg or "<Response [404]" in error_msg:
            logger.error(
                f"  ğŸ’¡ å¤±è´¥åŸå› : æ¨¡å‹ {model_config['model_id']} ä¸åœ¨ModelScopeå¹³å°ä¸Š"
            )
            logger.error(f"  ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            logger.error(f"    1. æ£€æŸ¥æ¨¡å‹IDæ˜¯å¦æ­£ç¡®")
            logger.error(f"    2. ç¡®è®¤æ¨¡å‹æ˜¯å¦å·²å‘å¸ƒåˆ°ModelScope")
            logger.error(f"    3. è€ƒè™‘ä»å…¶ä»–å¹³å°ï¼ˆå¦‚Hugging Faceï¼‰æ‰‹åŠ¨ä¸‹è½½")
            logger.error(f"    4. å°†è¯¥æ¨¡å‹çš„need_downloadå‚æ•°è®¾ç½®ä¸ºFalseä»¥è·³è¿‡ä¸‹è½½")

        # æ¸…ç†å·²åˆ›å»ºçš„æ¨¡å‹æ–‡ä»¶å¤¹
        if os.path.exists(model_dir):
            try:
                shutil.rmtree(model_dir)
                logger.info(f"  âœ“ å·²æ¸…ç†ä¸‹è½½å¤±è´¥çš„æ¨¡å‹æ–‡ä»¶å¤¹: {model_dir}")
            except Exception as cleanup_error:
                logger.error(
                    f"  âœ— æ¸…ç†æ¨¡å‹æ–‡ä»¶å¤¹å¤±è´¥: {model_dir}, é”™è¯¯: {cleanup_error}"
                )

        return False


def main(verify_level="full"):
    """
    ä¸»å‡½æ•°ï¼Œéå†æ‰€æœ‰æ¨¡å‹ï¼Œæ£€æŸ¥å¹¶ä¸‹è½½

    Args:
        verify_level (str): éªŒè¯çº§åˆ«ï¼Œå¯é€‰å€¼ï¼š
            - "basic": ä»…æµ‹è¯•å¯¼å…¥
            - "full": æµ‹è¯•å¯¼å…¥å’ŒåŠŸèƒ½
    """
    logger.info("å¼€å§‹å¤„ç†æ¨¡å‹ä¸‹è½½")
    logger.info(f"å…±éœ€å¤„ç† {len(MODELS_CONFIG)} ä¸ªæ¨¡å‹")
    logger.info(f"éªŒè¯çº§åˆ«: {verify_level}")

    # ç»Ÿè®¡ä¿¡æ¯
    total_models = len(MODELS_CONFIG)
    skip_count = 0
    skip_download_disabled_count = 0
    download_count = 0
    success_count = 0
    fail_count = 0

    for i, model_config in enumerate(MODELS_CONFIG, 1):
        logger.info(f"\n=== å¤„ç†æ¨¡å‹ {i}/{total_models}: {model_config['name']} ===")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
        if not model_config.get("need_download", True):
            logger.info(f"â­ï¸  æ¨¡å‹ä¸‹è½½å·²ç¦ç”¨ï¼Œè·³è¿‡: {model_config['name']}")
            skip_download_disabled_count += 1
            continue

        # 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if check_model_exists(model_config["local_dir"]):
            logger.info(f"âœ“ æ¨¡å‹æ–‡ä»¶å·²å­˜åœ¨: {model_config['local_dir']}")

            # 2. æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¯¼å…¥
            try:
                import_success, func_test_success = test_model_import(
                    model_config, verify_level
                )

                if import_success:
                    logger.info(f"âœ“ æ¨¡å‹å¯ä»¥æ­£å¸¸å¯¼å…¥ï¼Œè·³è¿‡ä¸‹è½½")
                    skip_count += 1
                    # åŠŸèƒ½æµ‹è¯•å¤±è´¥ä¸å½±å“è·³è¿‡ä¸‹è½½
                    if not func_test_success:
                        logger.warning(
                            f"âš ï¸  æ¨¡å‹å¯¼å…¥æˆåŠŸï¼Œä½†åŠŸèƒ½æµ‹è¯•å¤±è´¥ (ä¸å½±å“æ¨¡å‹ä½¿ç”¨)"
                        )
                    continue
                else:
                    logger.warning(f"âš ï¸  æ¨¡å‹å­˜åœ¨ä½†å¯¼å…¥å¤±è´¥ï¼Œé‡æ–°ä¸‹è½½")
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä¾èµ–ç¼ºå¤±å¯¼è‡´çš„é”™è¯¯
                error_msg = str(e).lower()
                if any(
                    keyword in error_msg
                    for keyword in [
                        "not found in your environment",
                        "requires the",
                        "missing",
                        "no module named",
                    ]
                ):
                    logger.error(f"âœ— æ¨¡å‹å¯¼å…¥å¤±è´¥ï¼ŒåŸå› : ç¼ºå°‘ä¾èµ–åº“")
                    logger.error(f"  é”™è¯¯ä¿¡æ¯: {e}")
                    logger.error(f"  è§£å†³æ–¹æ³•: å®‰è£…ç¼ºå°‘çš„ä¾èµ–åº“åé‡æ–°è¿è¡Œ")
                    fail_count += 1
                    continue  # è·³è¿‡é‡æ–°ä¸‹è½½ï¼Œå› ä¸ºä¾èµ–é—®é¢˜é‡æ–°ä¸‹è½½ä¹Ÿæ— æ³•è§£å†³
                else:
                    logger.warning(f"âš ï¸  æ¨¡å‹å­˜åœ¨ä½†å¯¼å…¥å¤±è´¥ï¼Œé‡æ–°ä¸‹è½½")
        else:
            logger.info(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½")

        # 3. ä¸‹è½½æ¨¡å‹
        download_count += 1
        if download_model(model_config):
            success_count += 1
        else:
            fail_count += 1

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n=== æ¨¡å‹å¤„ç†å®Œæˆ ===")
    logger.info(f"æ€»æ¨¡å‹æ•°: {total_models}")
    logger.info(f"è·³è¿‡ä¸‹è½½(å·²ç¦ç”¨): {skip_download_disabled_count}")
    logger.info(f"è·³è¿‡ä¸‹è½½(å·²å­˜åœ¨ä¸”å¯ç”¨): {skip_count}")
    logger.info(f"ä¸‹è½½å°è¯•: {download_count}")
    logger.info(f"ä¸‹è½½æˆåŠŸ: {success_count}")
    logger.info(f"ä¸‹è½½å¤±è´¥: {fail_count}")
    logger.info("æ¨¡å‹å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    import argparse

    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="æ¨¡å‹ä¸‹è½½å’ŒéªŒè¯è„šæœ¬")
    parser.add_argument(
        "--verify-level",
        type=str,
        choices=["basic", "full"],
        default="full",
        help="éªŒè¯çº§åˆ«: basic(ä»…æµ‹è¯•å¯¼å…¥), full(æµ‹è¯•å¯¼å…¥å’ŒåŠŸèƒ½)",
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è°ƒç”¨ä¸»å‡½æ•°
    main(verify_level=args.verify_level)
